attention_dropout: 0.1
batch_size: 16
batch_type: sents
dec_layers: 2
dec_rnn_size: 100
decay_method: none
decay_steps: 10000
decoder_type: rnn
dropout: 0.3
early_stopping: 0
enc_layers: 2
enc_rnn_size: 100
encoder_type: rnn
param_init_glorot: True
copy_attn: True
exp: ''
exp_host: ''
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
generator_function: softmax
global_attention: general
global_attention_function: softmax
input_feed: 1
keep_checkpoint: -1
layers: -1
learning_rate: 0.0005
learning_rate_decay: 0.5
max_grad_norm: 5
max_relative_positions: 0
model_dtype: fp32
normalization: sents
optim: adagrad
pool_factor: 8192
queue_size: 400
report_every: 50
reset_optim: none
rnn_size: -1
rnn_type: LSTM
sample_rate: 16000
save_checkpoint_steps: 10000
seed: 2019
src_word_vec_size: 50
tgt_word_vec_size: 50
train_from: ''
train_steps: 7000
valid_batch_size: 16
warmup_steps: 400
word_vec_size: -1
world_size: 1
gpu_ranks: 0
